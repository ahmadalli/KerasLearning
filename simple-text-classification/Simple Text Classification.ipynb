{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Simple Text Classification using Keras Deep Learning Python Library](https://www.opencodez.com/python/text-classification-using-keras.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from files to Python variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(1237)\n",
    "\n",
    "# Source file directory\n",
    "path_train = \"20news-bydate\\\\20news-bydate-train\"\n",
    "\n",
    "files_train = skds.load_files(path_train,load_content=False)\n",
    "\n",
    "label_index = files_train.target\n",
    "label_names = files_train.target_names\n",
    "labelled_files = files_train.filenames\n",
    "\n",
    "data_tags = [\"filename\",\"category\",\"news\"]\n",
    "data_list = []\n",
    "\n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],Path(f).read_text()))\n",
    "    i += 1\n",
    "\n",
    "# We have training data available as dictionary filename, category, data\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case data is not available as CSV. We have text data file and the directory in which the file is kept is our label or category. So we will first iterate through the directory structure and create data set that can be further utilized in training our model.\n",
    "\n",
    "We will use scikit-learn load_files method. This method can give us raw data as well as the labels and label indices. For our example, we will not load data at one go. We will iterate over files and prepare a [DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n",
    "\n",
    "At the end of above code, we will have a data frame that has a filename, category, actual data.\n",
    "\n",
    "**Note**: The above approach to make data available for training worked, as its volume is not huge. If you need to train on huge dataset then you have to consider BatchGenerator approach. In this approach, the data will be fed to your model in small batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data for Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take 80% data as training and remaining 20% for test.\n",
    "train_size = int(len(data) * .8)\n",
    "\n",
    "train_posts = data['news'][:train_size]\n",
    "train_tags = data['category'][:train_size]\n",
    "train_files_names = data['filename'][:train_size]\n",
    "\n",
    "test_posts = data['news'][train_size:]\n",
    "test_tags = data['category'][train_size:]\n",
    "test_files_names = data['filename'][train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will keep 80% of our data for training and remaining 20% for testing and validations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Prepare Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 news groups\n",
    "num_labels = 20\n",
    "vocab_size = 15000\n",
    "batch_size = 100\n",
    "\n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we classify texts we first pre-process the text using [Bag Of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) method. Now the keras comes with inbuilt [Tokenizer](https://keras.io/preprocessing/text/) which can be used to convert your text into a numeric vector. The text_to_matrix method above does exactly same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing Output Labels / Classes\n",
    "\n",
    "As we have converted our text to numeric vectors, we also need to make sure our labels are represented in the numeric format accepted by neural network model. The prediction is all about assigning the probability to each label.  We need to convert our labels to [one hot vector](https://en.wikipedia.org/wiki/One-hot)\n",
    "\n",
    "scikit-learn has a [LabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) class which makes it easy to build these one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Keras Model and Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               7680512   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 7,953,428\n",
      "Trainable params: 7,953,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8145 samples, validate on 906 samples\n",
      "Epoch 1/30\n",
      "8145/8145 [==============================] - 5s 651us/step - loss: 1.1899 - acc: 0.6840 - val_loss: 0.4666 - val_acc: 0.8720\n",
      "Epoch 2/30\n",
      "8145/8145 [==============================] - 2s 251us/step - loss: 0.1395 - acc: 0.9704 - val_loss: 0.4112 - val_acc: 0.8962\n",
      "Epoch 3/30\n",
      "8145/8145 [==============================] - 2s 250us/step - loss: 0.0519 - acc: 0.9925 - val_loss: 0.4064 - val_acc: 0.9018\n",
      "Epoch 4/30\n",
      "8145/8145 [==============================] - 2s 250us/step - loss: 0.0415 - acc: 0.9946 - val_loss: 0.4353 - val_acc: 0.9084\n",
      "Epoch 5/30\n",
      "8145/8145 [==============================] - 2s 257us/step - loss: 0.0401 - acc: 0.9947 - val_loss: 0.4757 - val_acc: 0.8951\n",
      "Epoch 6/30\n",
      "8145/8145 [==============================] - 2s 256us/step - loss: 0.0510 - acc: 0.9929 - val_loss: 0.4756 - val_acc: 0.9007\n",
      "Epoch 7/30\n",
      "8145/8145 [==============================] - 2s 250us/step - loss: 0.0307 - acc: 0.9968 - val_loss: 0.4525 - val_acc: 0.9062\n",
      "Epoch 8/30\n",
      "8145/8145 [==============================] - 2s 249us/step - loss: 0.0324 - acc: 0.9964 - val_loss: 0.4633 - val_acc: 0.9117\n",
      "Epoch 9/30\n",
      "8145/8145 [==============================] - 2s 247us/step - loss: 0.0422 - acc: 0.9951 - val_loss: 0.6202 - val_acc: 0.8830\n",
      "Epoch 10/30\n",
      "8145/8145 [==============================] - 2s 246us/step - loss: 0.0386 - acc: 0.9953 - val_loss: 0.5656 - val_acc: 0.8918\n",
      "Epoch 11/30\n",
      "8145/8145 [==============================] - 2s 248us/step - loss: 0.0407 - acc: 0.9955 - val_loss: 0.5050 - val_acc: 0.8996\n",
      "Epoch 12/30\n",
      "8145/8145 [==============================] - 2s 252us/step - loss: 0.0480 - acc: 0.9935 - val_loss: 0.6955 - val_acc: 0.8675\n",
      "Epoch 13/30\n",
      "8145/8145 [==============================] - 2s 253us/step - loss: 0.0653 - acc: 0.9905 - val_loss: 0.6822 - val_acc: 0.8797\n",
      "Epoch 14/30\n",
      "8145/8145 [==============================] - 2s 274us/step - loss: 0.0471 - acc: 0.9924 - val_loss: 0.6104 - val_acc: 0.8885\n",
      "Epoch 15/30\n",
      "8145/8145 [==============================] - 2s 270us/step - loss: 0.0333 - acc: 0.9962 - val_loss: 0.6261 - val_acc: 0.8885\n",
      "Epoch 16/30\n",
      "8145/8145 [==============================] - 2s 258us/step - loss: 0.0380 - acc: 0.9961 - val_loss: 0.6615 - val_acc: 0.8819\n",
      "Epoch 17/30\n",
      "8145/8145 [==============================] - 2s 260us/step - loss: 0.0339 - acc: 0.9966 - val_loss: 0.7168 - val_acc: 0.8742\n",
      "Epoch 18/30\n",
      "8145/8145 [==============================] - 2s 260us/step - loss: 0.0376 - acc: 0.9958 - val_loss: 0.6871 - val_acc: 0.8841\n",
      "Epoch 19/30\n",
      "8145/8145 [==============================] - 2s 258us/step - loss: 0.0403 - acc: 0.9956 - val_loss: 0.7182 - val_acc: 0.8852\n",
      "Epoch 20/30\n",
      "8145/8145 [==============================] - 2s 257us/step - loss: 0.0362 - acc: 0.9959 - val_loss: 0.6985 - val_acc: 0.8874\n",
      "Epoch 21/30\n",
      "8145/8145 [==============================] - 2s 262us/step - loss: 0.0430 - acc: 0.9955 - val_loss: 0.7902 - val_acc: 0.8841\n",
      "Epoch 22/30\n",
      "8145/8145 [==============================] - 2s 258us/step - loss: 0.0553 - acc: 0.9934 - val_loss: 0.8288 - val_acc: 0.8720\n",
      "Epoch 23/30\n",
      "8145/8145 [==============================] - 2s 258us/step - loss: 0.0641 - acc: 0.9921 - val_loss: 0.7880 - val_acc: 0.8742\n",
      "Epoch 24/30\n",
      "8145/8145 [==============================] - 2s 259us/step - loss: 0.0453 - acc: 0.9952 - val_loss: 0.8198 - val_acc: 0.8698\n",
      "Epoch 25/30\n",
      "8145/8145 [==============================] - 2s 251us/step - loss: 0.0454 - acc: 0.9947 - val_loss: 0.9779 - val_acc: 0.8444\n",
      "Epoch 26/30\n",
      "8145/8145 [==============================] - 2s 250us/step - loss: 0.0966 - acc: 0.9885 - val_loss: 0.8587 - val_acc: 0.8620\n",
      "Epoch 27/30\n",
      "8145/8145 [==============================] - 2s 249us/step - loss: 0.0639 - acc: 0.9924 - val_loss: 0.8121 - val_acc: 0.8720\n",
      "Epoch 28/30\n",
      "8145/8145 [==============================] - 2s 249us/step - loss: 0.0603 - acc: 0.9921 - val_loss: 0.9444 - val_acc: 0.8698\n",
      "Epoch 29/30\n",
      "8145/8145 [==============================] - 2s 250us/step - loss: 0.0777 - acc: 0.9920 - val_loss: 0.9602 - val_acc: 0.8709\n",
      "Epoch 30/30\n",
      "8145/8145 [==============================] - 2s 250us/step - loss: 0.0511 - acc: 0.9944 - val_loss: 0.9784 - val_acc: 0.8709\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263/2263 [==============================] - 0s 168us/step\n",
      "Test accuracy: 0.8806893534487557\n",
      "20news-bydate\\20news-bydate-train\\alt.atheism\\53114\n",
      "Actual label:alt.atheism\n",
      "Predicted label: alt.atheism\n",
      "20news-bydate\\20news-bydate-train\\comp.graphics\\38666\n",
      "Actual label:comp.graphics\n",
      "Predicted label: comp.graphics\n",
      "20news-bydate\\20news-bydate-train\\sci.med\\58932\n",
      "Actual label:sci.med\n",
      "Predicted label: sci.med\n",
      "20news-bydate\\20news-bydate-train\\sci.crypt\\15212\n",
      "Actual label:sci.crypt\n",
      "Predicted label: sci.crypt\n",
      "20news-bydate\\20news-bydate-train\\comp.os.ms-windows.misc\\9695\n",
      "Actual label:comp.os.ms-windows.misc\n",
      "Predicted label: comp.os.ms-windows.misc\n",
      "20news-bydate\\20news-bydate-train\\rec.sport.baseball\\104482\n",
      "Actual label:rec.sport.baseball\n",
      "Predicted label: rec.sport.baseball\n",
      "20news-bydate\\20news-bydate-train\\soc.religion.christian\\20731\n",
      "Actual label:soc.religion.christian\n",
      "Predicted label: comp.graphics\n",
      "20news-bydate\\20news-bydate-train\\comp.graphics\\38583\n",
      "Actual label:comp.graphics\n",
      "Predicted label: comp.graphics\n",
      "20news-bydate\\20news-bydate-train\\rec.sport.hockey\\52638\n",
      "Actual label:rec.sport.hockey\n",
      "Predicted label: rec.sport.hockey\n",
      "20news-bydate\\20news-bydate-train\\rec.sport.hockey\\52636\n",
      "Actual label:rec.sport.hockey\n",
      "Predicted label: rec.sport.baseball\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "text_labels = encoder.classes_\n",
    "\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    print(test_files_names.iloc[i])\n",
    "    print('Actual label:' + test_tags.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File -> 20news-bydate\\20news-bydate-test\\comp.graphics\\38758 Predicted label: comp.graphics\n",
      "File -> 20news-bydate\\20news-bydate-test\\misc.forsale\\76115 Predicted label: comp.sys.ibm.pc.hardware\n",
      "File -> 20news-bydate\\20news-bydate-test\\soc.religion.christian\\21329 Predicted label: soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "# These are the labels we stored from our training\n",
    "# The order is very important here.\n",
    "\n",
    "labels = np.array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
    " 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
    " 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space',\n",
    " 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast',\n",
    " 'talk.politics.misc', 'talk.religion.misc'])\n",
    "\n",
    "test_files = [\"20news-bydate\\\\20news-bydate-test\\\\comp.graphics\\\\38758\",\n",
    "              \"20news-bydate\\\\20news-bydate-test\\\\misc.forsale\\\\76115\",\n",
    "              \"20news-bydate\\\\20news-bydate-test\\\\soc.religion.christian\\\\21329\"\n",
    "              ]\n",
    "x_data = []\n",
    "for t_f in test_files:\n",
    "    t_f_data = Path(t_f).read_text()\n",
    "    x_data.append(t_f_data)\n",
    "\n",
    "x_data_series = pd.Series(x_data)\n",
    "x_tokenized = tokenizer.texts_to_matrix(x_data_series, mode='tfidf')\n",
    "\n",
    "i=0\n",
    "for x_t in x_tokenized:\n",
    "    prediction = model.predict(np.array([x_t]))\n",
    "    predicted_label = labels[np.argmax(prediction[0])]\n",
    "    print(\"File ->\", test_files[i], \"Predicted label: \" + predicted_label)\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
