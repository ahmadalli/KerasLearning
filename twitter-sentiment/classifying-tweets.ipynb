{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Classifying Tweets with Keras and TensorFlow](https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing our data\n",
    "\n",
    "We‚Äôre using the [Twitter Sentiment Analysis Dataset](http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/) available via ThinkNook. Be prepared; this dataset is extremely large and may take forever-ish to open in Excel (I had more success with Numbers).\n",
    "\n",
    "The only columns we‚Äôre interested in here are 1 and 3 ‚Äî we‚Äôll be training our net on inputs of column `SentimentText` with outputs of `Sentiment`.\n",
    "\n",
    "We need to convert `SentimentText` utterances to one-hot matrices, and create a dictionary of all the words we keep track of. Here, the `numpy` library is your friend. I hadn‚Äôt used it much before this but it‚Äôs super powerful and has some really useful built-in utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# extract data from a csv\n",
    "# notice the cool options to skip lines at the beginning\n",
    "# and to only take data from certain columns\n",
    "training = np.genfromtxt('Sentiment Analysis Dataset.csv', delimiter=',', skip_header=1, usecols=(1, 3), dtype=None, encoding='utf-8')\n",
    "\n",
    "# create our training data from the tweets\n",
    "train_x = [x[1] for x in training]\n",
    "# index all the sentiment labels\n",
    "train_y = np.asarray([x[0] for x in training])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we‚Äôve indexed all of our data; time to use Keras to make it machine-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# only work with the 3000 most popular words found in our dataset\n",
    "max_words = 3000\n",
    "\n",
    "# create a new Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# feed our tweets to the Tokenizer\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "# Tokenizers come with a convenient list of words and IDs\n",
    "dictionary = tokenizer.word_index\n",
    "# Let's save this out so we can use it later\n",
    "with open('dictionary.json', 'w') as dictionary_file:\n",
    "    json.dump(dictionary, dictionary_file)\n",
    "\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    # one really important thing that `text_to_word_sequence` does\n",
    "    # is make all texts the same length -- in this case, the length\n",
    "    # of the longest text in the set.\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
    "\n",
    "allWordIndices = []\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\n",
    "for text in train_x:\n",
    "    wordIndices = convert_text_to_index_array(text)\n",
    "    allWordIndices.append(wordIndices)\n",
    "\n",
    "# now we have a list of all tweets converted to index arrays.\n",
    "# cast as an array for future usage.\n",
    "allWordIndices = np.asarray(allWordIndices)\n",
    "\n",
    "# create one-hot matrices out of the indexed tweets\n",
    "train_x = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
    "# treat the labels as categories\n",
    "train_y = keras.utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ú® Cooooooooooooool. ‚ú® Now you have training data and labels that you‚Äôll be able to pipe right into your neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a model\n",
    "\n",
    "Keras makes building neural nets as simple as possible, to the point where you can add a layer to the network in short line of code. Here‚Äôs how I built my net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks simple enough. What does it mean?? üåàüåà\n",
    "\n",
    "Keras‚Äô `Sequential()` is a simple type of neural net that consists of a ‚Äústack‚Äù of layers executed in order.\n",
    "\n",
    "If we wanted to, we could make a stack of only two layers (input and output) to make a complete neural net ‚Äî without hidden layers, it wouldn‚Äôt be considered a deep neural net.\n",
    "\n",
    "The input and output layers are the most important, since they determine the overall shape of the neural net. You need to know what kind of input to expect, and what kind of output you want.\n",
    "\n",
    "A diagram of a neural net used to identify digits using MNIST data.\n",
    "A representation of a neural net for identifying digits using the MNIST dataset. Source\n",
    "\n",
    "Out network will mostly consist of `Dense` layers ‚Äî the ‚Äústandard‚Äù, linear neural net layer of inputs, weights, and outputs.\n",
    "\n",
    "In our case, we‚Äôre inputting a sentence that will be converted to a one-hot matrix of length `max_words` ‚Äî here, 3000. We also include how many outputs we want to come out of that layer (512, for funsies) and what kind of maximization (or ‚Äúactivation‚Äù) function to use.\n",
    "\n",
    "Activation functions are used when training the network; they tell the network how to judge when a weight for a particular node has created a good fit. In the first layer, I use `relu` (also for funsies). Activation functions differ, mostly in speed, but all the ones available in Keras and TensorFlow are viable; feel free to play around with them. If you don‚Äôt explicitly add an activation function, that layer will use a linear one.\n",
    "\n",
    "Our output layer consists of two possible outputs, since that‚Äôs how many categories our data could get sorted into. If you use a neural net to predict rather than classify, you‚Äôre actually creating a neural net with one possible output ‚Äî the prediction.\n",
    "\n",
    "In between the input and output layers, we have one more `Dense` layer and two `Dropout` layers. Dropouts are used to randomly remove data, which can help avoid overfitting. Overfitting can happen when you keep training on the same or overly-similar data ‚Äî as you train, your accuracy will hold steady or drop instead of rising.\n",
    "\n",
    "As the last step before training, we need to compile the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying that we want to collect the `accuracy` metric will give us really helpful live output as we train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train your network\n",
    "\n",
    "This is some tiny code that will take a while to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x, train_y,\n",
    "  batch_size=32,\n",
    "  epochs=5,\n",
    "  verbose=1,\n",
    "  validation_split=0.1,\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We‚Äôre fitting (training) our model off of inputs `train_x` and categories `train_y`. We evaluate data in groups of `batch_size`, checking the network‚Äôs accuracy, tweaking node weights, and then running through another batch. Small batches let you train networks much more quickly than if you tried to use a batch the size of your entire training dataset.\n",
    "\n",
    "`epochs` is how many times you do this batch-by-batch splitting. I‚Äôve found 5 to be good in this case; I tried 7, but ended up overfitting.\n",
    "\n",
    "`validation_split` says how much of your input you want to be reserved for testing data ‚Äî essential for seeing how accurate your network is at that point. Recommended training-to-test ratios are 80:20 or 90:10. You don‚Äôt want to compromise the size of your training corpus, but you need enough test data to actually see how your net is doing.\n",
    "\n",
    "Now go get coffee, or maybe a meal. And if you‚Äôre on a laptop, make sure it‚Äôs plugged in ‚Äî training can be a real battery killer ‚ò†Ô∏è\n",
    "\n",
    "An etching of a fractured geometric figure propped up on a platform surrounded by quare pyramids and a crucifix.\n",
    "If you need something to lose yourself in for about an hour, I suggest the Perspectiva Corporum Regularium.\n",
    "\n",
    "As you train the neural net, Keras will output running stats on what epoch you‚Äôre in, how much time is left in that epoch of training, and current accuracy. The value to watch is not `acc` but `val_acc`, or Validation Accuracy. This is your neural net's score when predicting values for data in your validation split.\n",
    "\n",
    "Your accuracy should start out low per epoch and rise throughout the epoch; it should increase at least a little across epochs. If your accuracy starts decreasing, you‚Äôre overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['negative', 'positive']\n",
    "\n",
    "def convert_text_to_index_array(text):\n",
    "    words = kpt.text_to_word_sequence(text)\n",
    "    wordIndices = []\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            wordIndices.append(dictionary[word])\n",
    "        else:\n",
    "            print(\"'%s' not in training corpus; ignoring.\" %(word))\n",
    "    return wordIndices\n",
    "\n",
    "# fill evalSentence for test\n",
    "evalSentence = \"it's sad\"\n",
    "testArr = convert_text_to_index_array(evalSentence)\n",
    "input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
    "# predict which bucket your input belongs in\n",
    "pred = model.predict(input)\n",
    "# and print it for the humons\n",
    "print(evalSentence)\n",
    "print(\"negative:\", pred[0][0])\n",
    "print(\"positive:\", pred[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "\n",
    "*this is not a part of original article*\n",
    "\n",
    "- use sgd instead of adam and player with epoch, batch_size and learning rate\n",
    "- use word2vec instead of one_hot\n",
    "- using RNNs instead of simple deep networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
